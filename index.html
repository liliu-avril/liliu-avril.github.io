
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">

<head>

  <title>Li Liu's Homepage</title>

  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="description" content="Li Liu is currently an assistant professor at the Hong Kong University of Science and Technology Guangzhou">
  <meta name="keywords" content="Liu Li, 刘李, liuli, Deep Learning, CUHK, Computer, Vision">
  <meta name="author" content="Li LIU" />

  <link rel="stylesheet" href="w3.css">

  <style>
  .w3-sidebar a {font-family: "Roboto", sans-serif}
  body,h1,h2,h3,h4,h5,h6,.w3-wide {font-family: "Montserrat", sans-serif;}
  </style>

  <link rel="icon" type="image/png" href="images/icons.jpeg">

</head>


<body class="w3-content" style="max-width:1000px">

<nav class="w3-sidebar w3-bar-block w3-black w3-collapse w3-top w3-right" style="z-index:3;width:150px" id="mySidebar">
  <div class="w3-container w3-display-container w3-padding-16">
    <h3><b>LIULI</b></h3>
  </div>
  <div class="w3-padding-64 w3-text-light-grey w3-large" style="font-weight:bold">
    <a href="#Home" class="w3-bar-item w3-button">Home</a>
    <a href="#News" class="w3-bar-item w3-button">News</a>
    <a href="#Publications" class="w3-bar-item w3-button">Publications</a>
    <a href="#Research Grants" class="w3-bar-item w3-button">Research Grants</a>
    <a href="#Teaching and Talks" class="w3-bar-item w3-button">Teaching and Talks</a>
    <a href="#Other Services" class="w3-bar-item w3-button">Other Services</a>
   <!-- <a href="#award" class="w3-bar-item w3-button">Awards</a>-->
  </div>
</nav>

<header class="w3-bar w3-top w3-hide-large w3-black w3-xlarge">
  <div class="w3-bar-item w3-padding-24">Li LIU</div>
  <a href="javascript:void(0)" class="w3-bar-item w3-button w3-padding-24 w3-right"  style="font-stretch: extra-expanded;" onclick="w3_open()"><b>≡</b></a>
  </div>
</header>

<div class="w3-overlay w3-hide-large" onclick="w3_close()" style="cursor:pointer" title="close side menu" id="myOverlay"></div>
<div class="w3-main" style="margin-left:150px">
  <div class="w3-hide-large" style="margin-top:83px"></div>

  <!--下面是个人简介部分,href连接的是需要跳转的网站-->
  <!--下面是个人简介部分,href连接的是需要跳转的网站-->
  <!--下面是个人简介部分,href连接的是需要跳转的网站-->
    <div class="w3-container w3-center w3-padding-32" id="Home">
      <img style="width: 80%;max-width: 320px" alt="profile photo" src="images/Liuli.jpeg">
      <h1>Li LIU (刘李)</h1>
        <p class="w3-justify" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;max-width:600px">
        I am an assistant professor at The Hong Kong University of Science and Technology (Guangzhou) in the AI Thrust. Previously, I graduated from Université Grenoble Alpes, Grenoble, France with a PhD, and I was a postdoc researcher at Ryerson University, Toronto, Canada. 
        <br> 
        <br> I worked on "AI for Social Good", and my research directions are few-shot learning (FSL), Security and Transferability of DNNs, and their applications to diverse application scenarios such as audio-visual Cued Speech recognition/generation and AI for Medical Imaging.
        
        
        </p>
        <p class="w3-center">
            <!--下面修改知乎，DBLP等链接-->
          <a href="mailto:liliu.math@gmail.com">Email</a> &nbsp/&nbsp
          <a href="https://scholar.google.com/citations?user=KQ2S01UAAAAJ&hl=en&oi=sra">Google Scholar</a> &nbsp/&nbsp
          <a href="https://www.zhihu.com/people/xiaomuzi-bianbian"> Zhi Hu </a> &nbsp/&nbsp
          <a href="https://github.com/liliu-avril"> GitHub </a>
        </p>
        </tbody></table>
  </div>
    <!--下面是新闻部分,href连接的是需要跳转的网站-->
    <!--下面是新闻部分,href连接的是需要跳转的网站-->
    <!--下面是新闻部分,href连接的是需要跳转的网站-->
  <div class="w3-container w3-light-grey w3-padding-32" id="News">
   <h2>News</h2>
      <p><li> <strong>I have several open positions for high quality PhD, master students, and research assistants. If you are interested, please contact me by liliu.math@gmail.com.</strong></p>
      <p><li> 12/2022, one paper have been accepted by <a href="https://arxiv.org/pdf/2201.07425.pdf">IEEE TPAMI 2022</a>. </li></p>
      <p><li> 12/2022, one paper have been accepted by <a href="https://arxiv.org/abs/2212.04097">TMI 2022</a>. </li></p>
   	  <p><li> 12/2022, Congratulations! Our two papers were awarded as <strong>"The Second Shenzhen Excellent Science and Technology Papers 2022"</strong> 
   	  <a href="https://saai.net.cn/2575.html">Excellent Science and Technology Papers</a>.</li></p>
      <p><li> 11/2022, one paper was published in <a href="https://openreview.net/pdf?id=wwW-1k1ljIg">NeurIPS 2022</a>.</li></p>
      <p><li> 10/2022, one paper was published in <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650171.pdf">ECCV 2022</a>.</li></p>
      <p><li> 06/2022, one paper was published in <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Feng_Boosting_Black-Box_Attack_With_Partially_Transferred_Conditional_Adversarial_Distribution_CVPR_2022_paper.pdf">CVPR 2022</a>.</li></p>
      <p><li> 03/2022, I was assigned as the Local Chair (China) <a href="https://2022.ieeeicassp.org/organizing_committee.html">ICASSP 2022</a>.</li></p>
      <p><li> 03/2022, two papers have been accepted by <a href="https://ieeexplore.ieee.org/xpl/conhome/9745891/proceeding">ICASSP 2022</a>.</li></p>


  </div>

 
    <!--下面添加publication内容，href为跳转链接-->
      <!--下面添加publication内容，href为跳转链接-->
      <!--下面添加publication内容，href为跳转链接-->
  <div class="w3-container w3-padding-32" id="Publications">
    <h2>Selected Publications </h2>

        <!--下面添加简单描述-->
          <!--下面添加简单描述-->
      
        <strong>Topic 1.</strong> I began to conduct research on the automatic recognition of French/English Cued Speech (CS) during the PhD and Postdoc periods. I proposed the first Mandarin Chinese CS System, which gains good feedback from international reviewers and CS society. By successfully introducing the deep neural network models into CS automatic recognition for the first time, and conducting research on the CS asynchronous fusion, my work has made great progress in solving the challenging problems in this field compared with the previous SOTA works.
          <!--下面添加单篇论文-->
      <p>
      <!--下面添加标题-->
      <li><strong>Re-synchronization using the Hand Preceding Model for Multi-modal Fusion in Automatic Continuous Cued Speech Recognition</strong>
            <!--下面添加作者-->
      <br>
      <strong>Li Liu</strong>, Gang Feng, Denis Beautemps, Xiao-Ping Zhang
      <br>
                  <!--下面添加会议和链接-->
                  <!--有pdf文件的放在data文件夹下改好文件名即可-->
      <em>IEEE Transactions on Multimedia</em> 2020 | <a style="color: #447ec9" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9016100">paper</a> 
      </p>

      <p>
      <li><strong>A Pilot Study on Mandarin Chinese Cued Speech</strong>
      <br>
      <strong>Li Liu</strong>, Gang Feng
      <br>
      <em>American Annals of the Deaf</em> 2019| <a style="color: #447ec9" href="https://muse.jhu.edu/article/745662/pdf">paper</a>
      </p>

      <p>
      <li><strong>Inner Lips Feature Extraction based on CLNF with Hybrid Dynamic Template for Cued Speech</strong>
      <br>
      <strong>Li Liu</strong>, Gang Feng, Denis Beautemps 
      <br>
      <em>EURASIP Journal on Image and Video Processing</em> 2017 | <a style="color: #447ec9" href="https://jivp-eurasipjournals.springeropen.com/articles/10.1186/s13640-017-0233-y">paper</a> 
      </p>

      <p>
      <li><strong>Automatic Temporal Segmentation of Hand Movements for Hand Positions Recognition in French Cued Speech</strong>
      <br>
      <strong>Li Liu</strong>, Gang Feng, Denis Beautemps
      <br>
      <em>International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</em> 2018 | <a style="color: #447ec9" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8462090">paper</a>
      </p>

      <p>
      <li><strong>Automatic Dynamic Template Tracking of Inner Lips based on CLNF</strong>
      <br>
      <strong>Li Liu</strong>, Gang Feng, Denis Beautemps
      <br>
      <em>International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</em> 2017 | <a style="color: #447ec9" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7953134">paper</a> 
      </p>

      <p>
      <li><strong>Automatic Detection of the Temporal Segmentation of Hand Movements in British English Cued Speech</strong>
      <br>
      <strong>Li Liu</strong>, Jianze Li, Gang Feng, Xiao-Ping Zhang
      <br>
      <em>Conference of the International Speech Communication Association (Interspeech)</em> 2019 | <a style="color: #447ec9" href="https://www.isca-speech.org/archive_v0/Interspeech_2019/pdfs/2353.pdf">paper</a> 

      <p>
      <li><strong>Visual Recognition of Continuous Cued Speech Using a Tandem CNN-HMM Approach</strong>
      <br>
      <strong>Li Liu</strong>, Thomas Hueber, Gang Feng, Denis Beautemps
      <br>
      <em>Conference of the International Speech Communication Association (Interspeech)</em> 2019 | <a style="color: #447ec9" href="https://www.isca-speech.org/archive_v0/Interspeech_2018/pdfs/2434.pdf">paper</a> 
      </p>

      <p>
      <li><strong>Objective Hand Complexity Comparison between Two Mandarin Chinese Cued Speech Systems</strong>
      <br>
      <strong>Li Liu</strong>, Gang Feng, Xiaoxi Ren, Xianping Ma
      <br>
      <em> International Symposium on Chinese Spoken Language Processing (ISCSLP)</em> 2022 | <a style="color: #447ec9" href="https://www.colips.org/conferences/iscslp2022/Proceedings/papers/ISCSLP2022_P022.pdf">paper</a>
      </p>

      <p>
      <li><strong>A Novel Resynchronization Procedure for Hand-lips Fusion applied to Continuous French Cued Speech Recognition</strong>
      <br>
      <strong>Li Liu</strong>, Gang Feng, Denis Beautemps, Xiao-Ping Zhang
      <br>
      <em>European Signal Processing Conference (EUSIPCO)</em> 2019 | <a style="color: #447ec9" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8903053">paper</a> 
      </p>

      <p>
      <li><strong>Inner Lips Parameter Estimation based on Adaptive Ellipse Model</strong>
      <br>
      <strong>Li Liu</strong>, Gang Feng, Denis Beautemps
      <br>
      <em>International Conference on Auditory-Visual Speech Processing (AVSP)</em> 2017 | <a style="color: #447ec9" href="https://www.isca-speech.org/archive_v0/AVSP_2017/pdfs/AVSP2017_paper_15.pdf">paper</a> 
      </p>

      <p>
      <li><strong>Extraction Automatique de Contour de Lèvre à partir du Modèle CLNF</strong>
      <br>
      <strong>Li Liu</strong>, Gang Feng, Denis Beautemps
      <br>
      <em>Journées d'Etudes sur la Parole (JEP)</em> 2016 | <a style="color: #447ec9" href="https://aclanthology.org/2016.jeptalnrecital-jep.39.pdf">paper</a> 

      <p>
      <li><strong>Acoustic-to-Articulatory Inversion based on Speech Decomposition and Auxiliary Feature</strong>
      <br>
      Jianrong Wang, Longxuan Zhao, Shanyu Wang, Ruiguo Yu, <strong>Li Liu*</strong>
      <br>
      <em> International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</em> 2022 <br> (* corresponding author) | <a style="color: #447ec9" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9746125">paper</a> 
      </p>

      <p>
      <li><strong>Residual-guided Personalized Speech Synthesis Based on Face Image</strong>
      <br>
      Jianrong Wang, Ge Zhang, Zhenyu Wu, Xuewei Li, <strong>Li Liu*</strong>
      <br>
      <em>International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</em> 2022 <br> (* corresponding author) | <a style="color: #447ec9" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9746808">paper</a> 
      </p>

      <p>
      <li><strong>Cross-Modal Knowledge Distillation Method for Automatic Cued Speech Recognition</strong>
      <br>
      Jianrong Wang, Ziyue Tang, Xuewei Li, Mei Yu, Qiang Fang, <strong>Li Liu*</strong>
      <br>
      <em>Conference of the International Speech Communication Association (Interspeech)</em> 2021 <br> (* corresponding author) | <a style="color: #447ec9" href="https://www.isca-speech.org/archive/pdfs/interspeech_2021/wang21v_interspeech.pdf">paper</a> 
      </p>

      <p>
      <li><strong>An Attention Self-supervised Contrastive Learning based Three-stage Model for Hand Shape Feature Representation in Cued Speech</strong>
      <br>
      Jianrong Wang, Nan Gu, Mei Yu, Xuewei Li, Qiang Fang, <strong>Li Liu*</strong>
      <br>
      <em>Conference of the International Speech Communication Association (Interspeech)</em> 2021 <br> (* corresponding author) | <a style="color: #447ec9" href="https://www.isca-speech.org/archive/pdfs/interspeech_2021/wang21f_interspeech.pdf">paper</a> 
      </p>

      <p>
      <li><strong>Three-Dimensional Lip Motion Network for Text-Independent Speaker Recognition</strong>
      <br>
      Jianrong Wang, Tong Wu, Shanyu Wang, Mei Yu, Qiang Fang, Ju Zhang, <strong>Li Liu*</strong>
      <br>
      <em> International Conference on Pattern Recognition (ICPR)</em> 2021 <br> (* corresponding author) | <a style="color: #447ec9" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9413218">paper</a> 
      </p>

      <p>
      <li><strong>MVNet: Memory Assistance and Vocal Reinforcement Network for Speech Enhancement</strong>
      <br>
      Jianrong Wang, Xiaomin Li, Xuewei Li, Mei Yu, Qiang Fang, <strong>Li Liu*</strong>
      <br>
      <em>International Conference on Neural Information Processing (ICONIP)</em> 2022 <br> (* corresponding author) | <a style="color: #447ec9" href="https://arxiv.org/pdf/2209.07302.pdf">paper</a> 
      </p>

      <strong>Topic 2.</strong> I have began to be involved in the AI Medical Imaging since November 2019. Based on clinical medical challenges encountered by the doctors in local hospitals, we investigated several AI based approaches (e.g., unsupervised learning, few-shot learning, data imbalance methods) to solve the Medical imaging related problems, especially the ultrasound imaging classification and pretraining. Besides, due to the specificity of medical AI, I also worked on the Robustness, Security and Transferability of DNNs.

      <p>
      <li><strong>WebUAV-3M: A Benchmark Unveiling the Power of Million-Scale Deep UAV Tracking</strong>
      <br>
      Chunhui Zhang, Guanjie Huang, <strong>Li Liu*</strong>, Shan Huang, Yinan Yang, Xiang Wan, Shiming Ge, Dacheng Tao
      <br>
      <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em> 2022 <br> (* corresponding author) | <a style="color: #447ec9" href="https://arxiv.org/pdf/2201.07425.pdf">paper</a> | <a style="color: #447ec9" href="https://github.com/983632847/WebUAV-3M">data and code</a>
      </p>

      <p>
      <li><strong>Contrastive Self-supervised Meta-Learning for pre-training Ultrasound Image Analysis Model</strong>
      <br>
      Yixiong Chen, Chunhui Zhang, Chris Ding, <strong>Li Liu*</strong>
      <br>
      <em>IEEE Transactions on Medical Imaging (TMI)</em> 2022 <br> (* corresponding author) | <a style="color: #447ec9" href="https://arxiv.org/abs/2212.04097">paper</a> | <a style="color: #447ec9" href="https://arxiv.org/pdf/2201.07425.pdf">paper</a> | <a style="color: #447ec9" href="https://github.com/Schuture/Meta-USCL">code</a>
      </p>

      <p>
      <li><strong>Pre-activation Distributions Expose Backdoor Neurons</strong>
      <br>
      Runkai Zheng, Rongjun Tang, Jianze Li, <strong>Li Liu*</strong>
      <br>
      <em>Conference on Neural Information Processing Systems (NeurIPS)</em> 2022 <br> (* corresponding author) | <a style="color: #447ec9" href="https://openreview.net/pdf?id=wwW-1k1ljIg">paper</a> 
      </p>

      <p>
      <li><strong>Data-free Backdoor Removal Based on Channel Lipschitzness</strong>
      <br>
      Runkai Zheng, Rongjun Tang, Jianze Li, <strong>Li Liu*</strong>
      <br>
      <em>European Conference on Computer Vision (ECCV)</em> 2022 <br> (* corresponding author) | <a style="color: #447ec9" href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650171.pdf">paper</a> | <a style="color: #447ec9" href="https://github.com/rkteddy/channel-Lipschitzness-based-pruning">code</a>
      </p>

      <p>
      <li><strong>CG-ATTACK: Modeling the Conditional Distribution of Adversarial Perturbations to Boost Black-Box Attack</strong>
      <br>
      Feng Yan, Baoyuan Wu, Yanbo Fan, <strong>Li Liu</strong>, Zhifeng Li, Shutao Xia
      <br>
      <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em> 2022 | <a style="color: #447ec9" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Feng_Boosting_Black-Box_Attack_With_Partially_Transferred_Conditional_Adversarial_Distribution_CVPR_2022_paper.pdf">paper</a> | <a style="color: #447ec9" href="https://github.com/Kira0096/CGATTACK">code</a>
      </p>

      <p>
      <li><strong>Effective Sample Pair Generation for Ultrasound Video Contrastive Representation Learning</strong>
      <br>
      JYixiong Chen, Chunhui Zhang, <strong>Li Liu*</strong>, Cheng Feng, Changfeng Dong, Yongfang Luo, Xiang Wan
      <br>
      <em> International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)</em> 2021  <br> (* corresponding author) | <a style="color: #447ec9" href="https://link.springer.com/chapter/10.1007/978-3-030-87237-3_60">paper</a><a style="color: red"> (13% oral)</a> | <a style="color: #447ec9" href="https://github.com/983632847/USCL">code</a>
      </p>

      <p>
      <li><strong>HiCo: Hierarchical Contrastive Learning for Ultrasound Video Model Pretraining</strong>
      <br>
      Chunhui Zhang, Yixiong Chen, <strong>Li Liu*</strong>, Qiong Liu, Xi Zhou
      <br>
      <em> Asian Conference on Computer Vision (ACCV)</em> 2022 <br> (* corresponding author) | <a style="color: #447ec9" href="https://openaccess.thecvf.com/content/ACCV2022/papers/Zhang_HiCo_Hierarchical_Contrastive__Learning_for_Ultrasound_Video_Model_Pretraining_ACCV_2022_paper.pdf">paper</a> | <a style="color: #447ec9" href="https://github.com/983632847/HiCo">code</a>
      </p>
        
      <p>
      <li><strong>Self-supervised Depth Estimation via Implicit Cues from Videos</strong>
      <br>
      Jianrong Wang, Ge Zhang, Zhenyu Wu, Xuewei Li, <strong>Li Liu*</strong>
      <br>
      <em> International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</em> 2021 <br> (* corresponding author) | <a style="color: #447ec9" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9413407">paper</a> 
      </p>

      <p>
      <li><strong>Multi-Modal Active Learning for Automatic Liver Fibrosis Diagnosis based on Ultrasound Shear Wave Elastography</strong>
      <br>
      Lufei Gao, Ruisong Zhou, Changfeng Dong, Cheng Feng, Zhen Li, Xiang Wan, <strong>Li Liu*</strong>
      <br>
      <em>IEEE International Symposium on Biomedical Imaging (ISBI)</em> 2021 <br> (* corresponding author) | <a style="color: #447ec9" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9434170">paper</a>
      </p>

      <p>
      <li><strong>Semi-Supervised Active Learning for COVID-19 Lung Ultrasound Multi-symptom Classification</strong>
      <br>
      Lei Liu, Wentao Lei, Yongfang Luo, Cheng Feng, Xiang Wan, <strong>Li Liu*</strong>
      <br>
      <em>International Conference on Tools with Artificial Intellignce (ICTAI)</em> 2020 <br> (* corresponding author) | <a style="color: #447ec9" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9288321">paper</a>
      </p>

      <div class="w3-container w3-padding-32" id="Research Grants">
      <h2>Research Grants </h2>
      <p><li><strong>Young Scientists Fund of the Natural Science Foundation of China (PI) </strong>
      <br>
      300k RMB, Grant No. 62101351, 2022-01 to 2024-12.
      </p>
      <p>
      <li><strong>Young Scientists Fund of the Natural Science Foundation of Guangdong Province (PI)</strong>
      <br>
      100k RMB, Grant No. 2020A1515110376, 2020-10 to 2023-09.
      <br>
      <p>
      <li><strong>Shenzhen Outstanding Scientific and Technological Innovation Talents PhD Startup Project (PI)</strong>
      <br>
      300k RMB, Grant No. RCBS20210609104447108, 2022-04 to 2024-04.
      <br>     	
      <p>
      <li><strong>Alibaba Innovative Research (PI)</strong>
      <br>
      500k RMB, 2022-04 to 2023-04.
      <br>
      <p>
      <li><strong>Tencent Technology Venture Philanthropy Program (PI)</strong>
      <br>
      300k RMB, 2022-06 to 2023-06. <a style="color: red"> (30 out of 307 projects)</a>
      <br>
     <p>
      <li><strong>Shenzhen Key Project-Basic Research Special Project (Natural Science Fund)</strong>
      <br>
      2000k RMB, 2023-01 to 2026-12. 
      <br>

      </ol>

    </p>
  </div>



  <!--下面添加talk内容，href为跳转链接-->
  <!--下面添加talk内容，href为跳转链接-->
  <!--下面添加talk内容，href为跳转链接-->
    <div class="w3-container w3-padding-32" id="Teaching and Talks">
  <!--<div class="w3-container w3-light-grey w3-padding-32" id="Teaching and talks">-->
    <h2>Teaching</h2>
      <p><li> <strong>Artificial Intelligence in Medical Imaging and Health (course code: CSC4080 and MDS6001)</strong> <br> The Chinese University of Hong Kong, Shenzhen (CUHK-SZ), Jan. - May 2022</a>.</li></p>



  </div>


<!--下面是services部分,href连接的是需要跳转的网站-->
  <div class="w3-container w3-padding-32" id="Other Services">
    <h2>Other Services</h2>
      <p><li> ICASSP 2022, China-site Technical Chair.</p>
      <p><li> Member of the Institute of Electrical and Electronics Engineers (IEEE Member).</p>
      <p><li> Member of the Pattern Recognition and Machine Intelligence Committee of the Chinese Society of Automation.</p>
      <p><li> Member of the Speech Dialogue and Auditory Processing Committee of CCF.</p>
      <p><li> Member of the Women Science and Technology within the Chinese Society of Image and Graphics.</p>
      <p><li> Reviewer for IEEE Signal Processing Magazine, IEEE/ACM Transactions on Audio, Speech and Language Processing, CVPR, ICASSP, Interspeech, IJCAI. </p>
  </div>

<!--下面是award部分,href连接的是需要跳转的网站-->
  <!--<div class="w3-container w3-padding-32" id="award">
    <h2>Awards</h2>
    <p><li> 2020, <a href="https://mp.weixin.qq.com/s/dORL01lgFNDHgjp3KMJmiQ">Nomination for Outstanding Youth Paper Award</a>, <a href="https://worldaic.com.cn/portal/en/aboutus.html">WAIC</a></p>               
    <p><li> 2017, <a href="https://research.google/outreach/phd-fellowship/recipients/?category=2017">Google PhD Fellowship</a></p>
    <p><li> 2017, <a href="http://scholarship.baidu.com/">Baidu Scholarship</a></p>
    <p><li> 2017, President's PhD Scholarship, Peking University</p>
    <p><li> 2017, National Scholarship for Graduate Students</p>
    <p><li> 2016, National Scholarship for Graduate Students</p>-->
 


 


  </div>

  <!-- End page content -->
</div>

<script>
// Accordion 
function myAccFunc() {
  var x = document.getElementById("demoAcc");
  if (x.className.indexOf("w3-show") == -1) {
    x.className += " w3-show";
  } else {
    x.className = x.className.replace(" w3-show", "");
  }
}

// Click on the "Jeans" link on page load to open the accordion for demo purposes
document.getElementById("myBtn").click();


// Open and close sidebar
function w3_open() {
  document.getElementById("mySidebar").style.display = "block";
  document.getElementById("myOverlay").style.display = "block";
}
 
function w3_close() {
  document.getElementById("mySidebar").style.display = "none";
  document.getElementById("myOverlay").style.display = "none";
}
</script>

</body>
</html>
